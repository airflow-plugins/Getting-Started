{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Big Query in Astronomer Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a local version of Astronomer\n",
    "\n",
    "### Cloud\n",
    "Run `astro init` to initiatlize a project, and then `astro airflow up` to start a dev environment. \n",
    "\n",
    "### Enterprise\n",
    "Run `astro airflow init` to initiatlize a project, and then `astro airflow start` to start a dev environment. You can put any custom python in `requirements.txt` and any OS-level packages in `packages.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the corresponding BigQuery Plugin\n",
    "\n",
    "### Cloud\n",
    "You'll need to clone this modified plugin into your project directory:\n",
    "\n",
    "`git clone https://github.com/airflow-plugins/bigquery_plugin`\n",
    "\n",
    "**Note:** _You'll need this extra step because Astronomer Cloud is currently on Airflow 1.8 - once 1.10 releases and we update our cloud image to pull from there, you'll be able to skip this step. To make up for this, there's some additional logging in the custom plugin courtsey of @andrewm4894\n",
    "\n",
    "Your project directory should look like this:\n",
    "\n",
    "![title](img/gcp_cloud_folder_structure.png)\n",
    "\n",
    "\n",
    "Now you can import it as you would any other plugin or package:\n",
    "\n",
    "```\n",
    "from plugins.bigquery_plugin.operators.custom_big_query_operator import CustomBigQueryOperator\n",
    "```\n",
    "\n",
    "### Enterprise:\n",
    "You can just import the plugin from _airflow.contribs_\n",
    "\n",
    "```\n",
    "airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter the contents of your keyfile in the Admin Panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In your Airflow UI (localhost 8080), navigate to Admin -> Connections.\n",
    "\n",
    "If you're on Cloud, you'll put the contents of your keyfile in the `Keyfile Path` field. In Enterprise Edition, put the contents in the `Keyfile JSON` field.\n",
    "\n",
    "![title](img/connections.png)\n",
    "\n",
    "Create a new connection\n",
    "![title](img/google_cloud_connection.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer to your GCP connection in your DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from plugins.bigquery_plugin.operators.custom_big_query_operator import CustomBigQueryOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2018, 1, 30),\n",
    "}\n",
    "\n",
    "dag = DAG('google_cloud_example',\n",
    "          default_args=default_args,\n",
    "          schedule_interval='@once')\n",
    "\n",
    "\n",
    "with dag:\n",
    "    \n",
    "    kick_off_dag=DummyOperator(task_id='kick_off_dag')\n",
    "\n",
    "    bigquery_task = CustomBigQueryOperator(\n",
    "            task_id='execute_query'\n",
    "            bql=\"YOUR SQL HERE\",\n",
    "            destination_dataset_table=my_target_table_full,\n",
    "            write_disposition=\"WRITE_TRUNCATE\",\n",
    "            bigquery_conn_id='google_cloud_Id')\n",
    "    \n",
    "    kick_off_dag >> bigquery_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enterprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2018, 1, 30),\n",
    "}\n",
    "\n",
    "dag = DAG('google_cloud_example',\n",
    "          default_args=default_args,\n",
    "          schedule_interval='@once')\n",
    "\n",
    "\n",
    "with dag:\n",
    "    \n",
    "    kick_off_dag=DummyOperator(task_id='kick_off_dag')\n",
    "\n",
    "    bigquery_task = BigQueryOperator(\n",
    "            task_id='execute_query',\n",
    "            bql=\"YOUR SQL HERE\",\n",
    "            destination_dataset_table=my_target_table_full,\n",
    "            write_disposition=\"WRITE_TRUNCATE\",\n",
    "            bigquery_conn_id='google_cloud_Id')\n",
    "    \n",
    "    kick_off_dag >> bigquery_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud:\n",
    "`astro deploy`\n",
    "\n",
    "### Enterprise:\n",
    "`astro airflow deploy  YOURNAMESPACE`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Examples:\n",
    "\n",
    "We're always working to improve our documentation, so feel check back soon for more examples!\n",
    "\n",
    "We've also found some good ones around the internet:\n",
    "- https://github.com/alexvanboxel/airflow-gcp-examples\n",
    "- http://engineering.pmc.com/2017/03/playing-around-with-apache-airflow-bigquery-62/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
